apiVersion: template.openshift.io/v1
kind: Template
metadata:
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/modelServingSupport: '["multi"]'
  labels:
    opendatahub.io/dashboard: "true"
  name: triton-24.11-py3-template
  namespace: redhat-ods-applications
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      annotations:
        enable-route: "true"
        maxLoadingConcurrency: "1"
        opendatahub.io/accelerator-name: ""
        opendatahub.io/apiProtocol: REST
        opendatahub.io/template-display-name: Triton runtime 24.11-py3
        opendatahub.io/template-name: triton-24.11-py3
        openshift.io/display-name: netsentinel-trition-24.11-py3
      labels:
        name: triton-24.11-py3
        opendatahub.io/dashboard: "true"
      name: triton-24.11-py3
    spec:
      builtInAdapter:
        memBufferBytes: 134217728
        modelLoadingTimeoutMillis: 90000
        runtimeManagementPort: 8001
        serverType: triton
      containers:
        - args:
            - -c
            - 'mkdir -p /models/_triton_models; chmod 777 /models/_triton_models; exec tritonserver
              "--model-repository=/models/_triton_models" "--model-control-mode=explicit"
              "--strict-model-config=false" "--strict-readiness=false" "--allow-http=true"
              "--allow-sagemaker=false" '
          command:
            - /bin/sh
          image: nvcr.io/nvidia/tritonserver:24.11-py3
          livenessProbe:
            exec:
              command:
                - curl
                - --fail
                - --silent
                - --show-error
                - --max-time
                - "9"
                - http://localhost:8000/v2/health/live
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 10
          name: triton
          resources:
            limits:
              cpu: "5"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 1Gi
          volumeMounts:
            - mountPath: /dev/shm
              name: shm
      grpcDataEndpoint: port:8001
      grpcEndpoint: port:8085
      multiModel: true
      protocolVersions:
        - grpc-v2
      supportedModelFormats:
        - autoSelect: true
          name: keras
          version: "2"
        - autoSelect: true
          name: onnx
          version: "1"
        - autoSelect: true
          name: pytorch
          version: "1"
        - autoSelect: true
          name: tensorflow
          version: "1"
        - autoSelect: true
          name: tensorflow
          version: "2"
        - autoSelect: true
          name: tensorrt
          version: "7"
      volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 2Gi
          name: shm
